{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hospital Stay Duration Prediction Model with temporal patterns\n",
        "\n",
        "This module implements a machine learning approach to predict hospital length of stay\n",
        "using baseline patient characteristics, vital sign statistics, diagnosis information,\n",
        "and temporal patterns.\n",
        "\n",
        "Dataset Tables:\n",
        "\n",
        "1. Vital Signs Dataset (over_20_records.csv):\n",
        "   - Contains patient measurements during hospitalization\n",
        "   - Key columns: PatientNum, PatientAgeAtAdmission, TotalHospitalDays, Execution_Date,\n",
        "     HoursFromAdmission, Parameter_Name, ResultValue, NumericResult, ReadingSequence\n",
        "   - Each row represents a single measurement for a specific patient and parameter\n",
        "\n",
        "2. Diagnoses Dataset (patients_diagnoses.csv):\n",
        "   - Contains patients' diagnostic information using ICD-9/ICD-10 codes\n",
        "   - Key columns: PatientNum, ICD9\n",
        "   - Each row represents a single diagnosis for a specific patient\n",
        "\n",
        "3. Temporal Patterns Dataset (train.csv):\n",
        "   - Contains extracted temporal patterns for each patient\n",
        "   - First column: PatientNum (labeled as 'id')\n",
        "   - Other columns: Various temporal patterns and their frequency for each patient\n",
        "\n",
        "Features used in the prediction:\n",
        "\n",
        "1. Baseline Patient Features:\n",
        "   - Age at admission\n",
        "   - BMI (body mass index)\n",
        "   - Weight\n",
        "\n",
        "2. Vital Sign Statistical Features:\n",
        "   For each vital sign (heart rate, blood pressure, temperature, etc.):\n",
        "   - Measurement frequency (count)\n",
        "   - Statistical summaries (mean, median, std, min, max, range, trend)\n",
        "\n",
        "3. Diagnosis-Based Features:\n",
        "   - Total diagnosis count\n",
        "   - Binary indicators for major disease categories\n",
        "   - Binary indicators for common specific diagnosis codes\n",
        "\n",
        "4. Temporal Pattern Features:\n",
        "   - Frequency of various temporal patterns for each patient\n",
        "\n",
        "The implementation follows these main steps:\n",
        "1. Data loading and cleaning\n",
        "2. Feature extraction and transformation\n",
        "3. Model training with multiple regression algorithms\n",
        "4. Performance evaluation and visualization\n",
        "5. Feature importance analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "lDiYcGEoySiz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM9zPMYqqf1w",
        "outputId": "0dbd6881-4479-4448-cc4b-8b8dab458846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing diagnosis features...\n",
            "Added diagnosis features\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Performance:\n",
            "  Mean Absolute Error: 44.78 days\n",
            "  Root Mean Squared Error: 74.13 days\n",
            "  R² Score: 0.27\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Performance:\n",
            "  Mean Absolute Error: 50.83 days\n",
            "  Root Mean Squared Error: 83.18 days\n",
            "  R² Score: 0.08\n",
            "\n",
            "Training AdaBoost...\n",
            "AdaBoost Performance:\n",
            "  Mean Absolute Error: 47.88 days\n",
            "  Root Mean Squared Error: 82.19 days\n",
            "  R² Score: 0.10\n",
            "\n",
            "Training Elastic Net...\n",
            "Elastic Net Performance:\n",
            "  Mean Absolute Error: 62.92 days\n",
            "  Root Mean Squared Error: 87.75 days\n",
            "  R² Score: -0.02\n",
            "\n",
            "Training Huber Regressor...\n",
            "Huber Regressor Performance:\n",
            "  Mean Absolute Error: 67.33 days\n",
            "  Root Mean Squared Error: 85.52 days\n",
            "  R² Score: 0.03\n",
            "\n",
            "Training SVR...\n",
            "SVR Performance:\n",
            "  Mean Absolute Error: 65.40 days\n",
            "  Root Mean Squared Error: 101.01 days\n",
            "  R² Score: -0.35\n",
            "\n",
            "Best model based on MAE: Random Forest\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.linear_model import ElasticNet, HuberRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Function to extract baseline patient features\n",
        "def extract_baseline_features(df):\n",
        "    \"\"\"Extract baseline features from patient data\"\"\"\n",
        "    # Group by patient to get unique patient records\n",
        "    patients_df = df.groupby('PatientNum').agg({\n",
        "        'PatientAgeAtAdmission': 'first',\n",
        "        'TotalHospitalDays': 'first'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Extract BMI and Weight as baseline features (using the latest value)\n",
        "    bmi_data = df[df['Parameter_Name'] == 'BMI'].sort_values('HoursFromAdmission')\n",
        "    weight_data = df[df['Parameter_Name'] == 'משקל'].sort_values('HoursFromAdmission')\n",
        "\n",
        "    # Create DataFrames with just the latest BMI and weight values\n",
        "    latest_bmi = bmi_data.groupby('PatientNum').last()[['NumericResult']].rename(\n",
        "        columns={'NumericResult': 'BMI'})\n",
        "    latest_weight = weight_data.groupby('PatientNum').last()[['NumericResult']].rename(\n",
        "        columns={'NumericResult': 'Weight'})\n",
        "\n",
        "    # Merge baseline features\n",
        "    patients_df = patients_df.merge(latest_bmi, on='PatientNum', how='left')\n",
        "    patients_df = patients_df.merge(latest_weight, on='PatientNum', how='left')\n",
        "\n",
        "\n",
        "    return patients_df\n",
        "\n",
        "# Function to process diagnosis data\n",
        "def prepare_diagnosis_features(diagnoses_df):\n",
        "    \"\"\"Process ICD9/ICD10 diagnoses into features for the model\"\"\"\n",
        "\n",
        "\n",
        "    # Convert patient number to numeric\n",
        "    if not pd.api.types.is_numeric_dtype(diagnoses_df['PatientNum']):\n",
        "        diagnoses_df['PatientNum'] = pd.to_numeric(diagnoses_df['PatientNum'], errors='coerce')\n",
        "    diagnoses_df = diagnoses_df.dropna(subset=['PatientNum'])\n",
        "    diagnoses_df['PatientNum'] = diagnoses_df['PatientNum'].astype(int)\n",
        "\n",
        "    # Handle ICD9 codes\n",
        "    diagnoses_df['ICD9'] = diagnoses_df['ICD9'].astype(str).str.strip()\n",
        "    diagnoses_df = diagnoses_df[diagnoses_df['ICD9'].str.len() > 0]\n",
        "    diagnoses_df['ICD9_category'] = diagnoses_df['ICD9'].str.extract(r'([A-Z]?[0-9]{1,3})')[0]\n",
        "\n",
        "\n",
        "\n",
        "    # Convert to string to prevent issues\n",
        "    diagnoses_df['ICD9_category'] = diagnoses_df['ICD9_category'].fillna('').astype(str)\n",
        "\n",
        "    # Create unique patient list\n",
        "    patient_diagnoses = pd.DataFrame(diagnoses_df['PatientNum'].unique(), columns=['PatientNum'])\n",
        "\n",
        "    # Add total diagnosis count\n",
        "    diagnosis_counts = diagnoses_df.groupby('PatientNum').size().reset_index(name='diagnosis_count')\n",
        "    patient_diagnoses = patient_diagnoses.merge(diagnosis_counts, on='PatientNum', how='left')\n",
        "\n",
        "    # Define disease categories\n",
        "    disease_categories = {\n",
        "        'infectious': ['001', '002', '003', '004', '005', '006', '007', '008', '009', '01', '02', '03', '04', '05', '06', '07', '08', '09', 'A', 'B'],\n",
        "        'neoplasms': ['14', '15', '16', '17', '18', '19', '20', '21', '22', '23', 'C', 'D0', 'D1', 'D2', 'D3', 'D4'],\n",
        "        'endocrine': ['24', '25', '26', '27', 'E'],\n",
        "        'blood': ['28', '29', 'D5', 'D6', 'D7', 'D8'],\n",
        "        'mental': ['29', '30', '31', 'F'],\n",
        "        'nervous': ['32', '33', '34', '35', '36', '37', 'G'],\n",
        "        'circulatory': ['39', '40', '41', '42', '43', '44', '45', 'I'],\n",
        "        'respiratory': ['46', '47', '48', '49', '50', '51', 'J'],\n",
        "        'digestive': ['52', '53', '54', '55', '56', '57', 'K'],\n",
        "        'genitourinary': ['58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', 'N'],\n",
        "        'pregnancy': ['63', '64', '65', '66', '67', 'O'],\n",
        "        'skin': ['68', '69', '70', '71', 'L'],\n",
        "        'musculoskeletal': ['71', '72', '73', 'M'],\n",
        "        'congenital': ['74', '75', 'Q'],\n",
        "        'perinatal': ['76', '77', 'P'],\n",
        "        'symptoms': ['78', '79', 'R'],\n",
        "        'injury': ['80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', 'S', 'T'],\n",
        "        'external_causes': ['E', 'V', 'W', 'X', 'Y'],\n",
        "        'factors': ['Z']\n",
        "    }\n",
        "\n",
        "    # Create mapping from ICD code to category\n",
        "    icd_to_category = {}\n",
        "    for category, codes in disease_categories.items():\n",
        "        for code in codes:\n",
        "            icd_to_category[code] = category\n",
        "\n",
        "    # Map each diagnosis to appropriate category\n",
        "    def get_prefix(icd_code):\n",
        "        if not icd_code or pd.isna(icd_code) or icd_code == '':\n",
        "            return ''\n",
        "        icd_code = str(icd_code).strip()\n",
        "        if len(icd_code) == 0:\n",
        "            return ''\n",
        "        if icd_code[0].isalpha():\n",
        "            return icd_code[0]  # First letter for ICD10 codes\n",
        "        else:\n",
        "            # Return first digit for ICD9 codes\n",
        "            return icd_code[:1]\n",
        "\n",
        "    # Apply safer function\n",
        "    diagnoses_df['prefix'] = diagnoses_df['ICD9_category'].apply(get_prefix)\n",
        "\n",
        "    # Map prefix to category\n",
        "    diagnoses_df['disease_category'] = diagnoses_df['prefix'].map(icd_to_category)\n",
        "    diagnoses_df['disease_category'] = diagnoses_df['disease_category'].fillna('other')\n",
        "\n",
        "\n",
        "    # Create flags for each disease category\n",
        "    for category in list(set(disease_categories.keys())) + ['other']:\n",
        "        category_counts = diagnoses_df[diagnoses_df['disease_category'] == category].groupby('PatientNum').size().reset_index(name=f'has_{category}')\n",
        "        patient_diagnoses = patient_diagnoses.merge(category_counts, on='PatientNum', how='left')\n",
        "        patient_diagnoses[f'has_{category}'] = patient_diagnoses[f'has_{category}'].fillna(0)\n",
        "\n",
        "        # Convert to binary flag\n",
        "        patient_diagnoses[f'has_{category}'] = (patient_diagnoses[f'has_{category}'] > 0).astype(int)\n",
        "\n",
        "    # Identify common diagnosis codes and add flags\n",
        "    valid_codes = diagnoses_df['ICD9_category'].dropna()\n",
        "    if len(valid_codes) > 0:\n",
        "        top_codes = valid_codes.value_counts().head(20).index\n",
        "\n",
        "        for code in top_codes:\n",
        "            if not pd.isna(code) and str(code).strip() != '':\n",
        "                code_counts = diagnoses_df[diagnoses_df['ICD9_category'] == code].groupby('PatientNum').size().reset_index(name=f'has_icd_{code}')\n",
        "                patient_diagnoses = patient_diagnoses.merge(code_counts, on='PatientNum', how='left')\n",
        "                patient_diagnoses[f'has_icd_{code}'] = patient_diagnoses[f'has_icd_{code}'].fillna(0)\n",
        "\n",
        "                # Convert to binary flag\n",
        "                patient_diagnoses[f'has_icd_{code}'] = (patient_diagnoses[f'has_icd_{code}'] > 0).astype(int)\n",
        "\n",
        "\n",
        "    return patient_diagnoses\n",
        "\n",
        "# Function to extract both counts and statistical summaries of vital sign measurements\n",
        "def extract_parameter_statistics(df):\n",
        "    \"\"\"Extract both counts and statistical summaries of vital sign measurements\"\"\"\n",
        "    parameters = ['דופק', 'לחץ סיסטולי', 'לחץ דיאסטולי', 'חום',\n",
        "                  'סטורציה', 'מספר נשימות', 'כאב', 'סוכר בדם']\n",
        "\n",
        "    patient_ids = df['PatientNum'].unique()\n",
        "    parameter_features = pd.DataFrame({'PatientNum': patient_ids})\n",
        "\n",
        "    for param in parameters:\n",
        "        param_data = df[df['Parameter_Name'] == param]\n",
        "\n",
        "        # Extract counts (monitoring frequency)\n",
        "        counts = param_data.groupby('PatientNum').size().reset_index(name=f'{param}_count')\n",
        "        parameter_features = parameter_features.merge(counts, on='PatientNum', how='left')\n",
        "        parameter_features[f'{param}_count'] = parameter_features[f'{param}_count'].fillna(0)\n",
        "\n",
        "        # For each patient, calculate statistics if they have measurements\n",
        "        stats_df = pd.DataFrame(columns=['PatientNum',\n",
        "                                         f'{param}_mean', f'{param}_median', f'{param}_std',\n",
        "                                         f'{param}_min', f'{param}_max', f'{param}_range', f'{param}_trend'])\n",
        "\n",
        "        for patient_id in patient_ids:\n",
        "            patient_param_data = param_data[param_data['PatientNum'] == patient_id]\n",
        "\n",
        "            if not patient_param_data.empty:\n",
        "                values = patient_param_data['NumericResult'].values\n",
        "\n",
        "                # Create a row with statistics\n",
        "                stats_row = {\n",
        "                    'PatientNum': patient_id,\n",
        "                    f'{param}_mean': np.mean(values),\n",
        "                    f'{param}_median': np.median(values),\n",
        "                    f'{param}_std': np.std(values) if len(values) > 1 else 0,\n",
        "                    f'{param}_min': np.min(values),\n",
        "                    f'{param}_max': np.max(values),\n",
        "                    f'{param}_range': np.max(values) - np.min(values),\n",
        "                    f'{param}_trend': values[-1] - values[0] if len(values) > 1 else 0\n",
        "                }\n",
        "\n",
        "                stats_df = pd.concat([stats_df, pd.DataFrame([stats_row])], ignore_index=True)\n",
        "\n",
        "        # Merge statistics with the main features dataframe\n",
        "        if not stats_df.empty:\n",
        "            parameter_features = parameter_features.merge(stats_df, on='PatientNum', how='left')\n",
        "\n",
        "            # Fill missing values with zeros for patients without measurements\n",
        "            for col in [f'{param}_mean', f'{param}_median', f'{param}_std',\n",
        "                        f'{param}_min', f'{param}_max', f'{param}_range', f'{param}_trend']:\n",
        "                parameter_features[col] = parameter_features[col].fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "    return parameter_features\n",
        "\n",
        "# Function to load and process temporal pattern features\n",
        "def load_temporal_patterns(patterns_file):\n",
        "    \"\"\"Load and process temporal pattern features from the patterns file\"\"\"\n",
        "    try:\n",
        "        # Read the first line to understand the format\n",
        "        with open(patterns_file, 'r') as f:\n",
        "            first_line = f.readline().strip()\n",
        "\n",
        "        # Check if the file is CSV format\n",
        "        if ',' in first_line:\n",
        "            # Try reading as CSV\n",
        "            patterns_df = pd.read_csv(patterns_file)\n",
        "\n",
        "            # If there's only one column, the CSV might be parsed incorrectly\n",
        "            if patterns_df.shape[1] == 1:\n",
        "                # Split the single column into multiple columns\n",
        "                first_col_name = patterns_df.columns[0]\n",
        "                patterns_df = patterns_df[first_col_name].str.split(',', expand=True)\n",
        "\n",
        "            # Rename columns for clarity\n",
        "            if patterns_df.shape[1] >= 2:\n",
        "                # Use the first row as header if it contains pattern information\n",
        "                if \"@@Pair\" in str(patterns_df.iloc[0, 0]):\n",
        "                    # Extract column names from first row\n",
        "                    column_names = patterns_df.iloc[0].tolist()\n",
        "                    patterns_df = patterns_df.iloc[1:]\n",
        "                else:\n",
        "                    # Generate default column names\n",
        "                    column_names = ['id']\n",
        "                    for i in range(1, patterns_df.shape[1]):\n",
        "                        column_names.append(f'pattern_{i}')\n",
        "\n",
        "                patterns_df.columns = column_names\n",
        "        else:\n",
        "            # Try space-delimited format as fallback\n",
        "            patterns_df = pd.read_csv(patterns_file, header=None, delim_whitespace=True)\n",
        "            column_names = ['id']\n",
        "            for i in range(1, patterns_df.shape[1]):\n",
        "                column_names.append(f'pattern_{i}')\n",
        "            patterns_df.columns = column_names\n",
        "\n",
        "        # Ensure the id column is properly set\n",
        "        if 'id' in patterns_df.columns:\n",
        "            # Rename to PatientNum for consistency\n",
        "            patterns_df = patterns_df.rename(columns={'id': 'PatientNum'})\n",
        "\n",
        "            # Ensure PatientNum is numeric\n",
        "            patterns_df['PatientNum'] = pd.to_numeric(patterns_df['PatientNum'], errors='coerce')\n",
        "            patterns_df = patterns_df.dropna(subset=['PatientNum'])\n",
        "            patterns_df['PatientNum'] = patterns_df['PatientNum'].astype(int)\n",
        "\n",
        "            # Convert all pattern columns to numeric\n",
        "            pattern_cols = [col for col in patterns_df.columns if col != 'PatientNum']\n",
        "            for col in pattern_cols:\n",
        "                patterns_df[col] = pd.to_numeric(patterns_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "\n",
        "        return patterns_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading temporal patterns: {e}\")\n",
        "        print(\"Detailed error information:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Function for hospitalization days prediction with all features\n",
        "def predict_hospitalization_days(df, patterns_df, diagnoses_df=None):\n",
        "    \"\"\"Predict hospitalization days using baseline, vital sign statistics, and temporal pattern features\"\"\"\n",
        "    # Extract baseline features\n",
        "    patients_df = extract_baseline_features(df)\n",
        "\n",
        "    # Extract parameter statistical features (includes counts and statistical summaries)\n",
        "    parameter_stats = extract_parameter_statistics(df)\n",
        "\n",
        "    # Merge baseline features with parameter statistics\n",
        "    X_df = patients_df.merge(parameter_stats, on='PatientNum', how='left')\n",
        "\n",
        "    # Merge with temporal patterns\n",
        "    # This will automatically filter to only include patients present in both datasets\n",
        "    X_df = X_df.merge(patterns_df, on='PatientNum', how='inner')\n",
        "\n",
        "    # Prepare diagnosis features if available\n",
        "    if diagnoses_df is not None:\n",
        "        print(\"Preparing diagnosis features...\")\n",
        "        diagnosis_features_df = prepare_diagnosis_features(diagnoses_df)\n",
        "\n",
        "        # Merge diagnosis features\n",
        "        X_df = X_df.merge(diagnosis_features_df, on='PatientNum', how='left')\n",
        "\n",
        "        # Fill missing values\n",
        "        X_df = X_df.fillna(0)\n",
        "\n",
        "        print(f\"Added diagnosis features\")\n",
        "\n",
        "    # Separate features and target\n",
        "    y = X_df['TotalHospitalDays'].values\n",
        "    patient_ids = X_df['PatientNum'].values\n",
        "    X = X_df.drop(['PatientNum', 'TotalHospitalDays'], axis=1)\n",
        "\n",
        "    # Store feature names for later\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    # Convert to numpy array\n",
        "    X = X.values\n",
        "\n",
        "    # Apply Winsorization - cap extreme values\n",
        "    upper_limit = 365  # Cap at 365 days (1 year)\n",
        "    y_capped = np.minimum(y, upper_limit)\n",
        "    capped_count = np.sum(y > upper_limit)\n",
        "\n",
        "    # Fill missing values and scale features\n",
        "    imputer = StandardScaler()\n",
        "    X_imputed = np.nan_to_num(X, nan=0)\n",
        "    X_scaled = imputer.fit_transform(X_imputed)\n",
        "\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train_capped, y_test_original, train_ids, test_ids = train_test_split(\n",
        "        X_scaled, y_capped, patient_ids, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Also keep the original uncapped values for test set to evaluate true error\n",
        "    y_test = y_test_original.copy()\n",
        "\n",
        "\n",
        "\n",
        "    # Define multiple models to compare\n",
        "    models = {\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "        'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
        "        'Elastic Net': ElasticNet(random_state=42, alpha=0.1, l1_ratio=0.5),\n",
        "        'Huber Regressor': HuberRegressor(epsilon=1.35),\n",
        "        'SVR': SVR(kernel='rbf', C=10.0, gamma='auto')\n",
        "    }\n",
        "\n",
        "    # Train all models\n",
        "    trained_models = {}\n",
        "    model_performances = {}\n",
        "    predictions = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        model.fit(X_train, y_train_capped)\n",
        "        trained_models[name] = model\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        predictions[name] = y_pred\n",
        "\n",
        "        # Evaluate performance\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        model_performances[name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R²': r2\n",
        "        }\n",
        "\n",
        "        print(f\"{name} Performance:\")\n",
        "        print(f\"  Mean Absolute Error: {mae:.2f} days\")\n",
        "        print(f\"  Root Mean Squared Error: {rmse:.2f} days\")\n",
        "        print(f\"  R² Score: {r2:.2f}\")\n",
        "\n",
        "    # Find the best model based on MAE\n",
        "    best_model_name = min(model_performances, key=lambda k: model_performances[k]['MAE'])\n",
        "    print(f\"\\nBest model based on MAE: {best_model_name}\")\n",
        "    best_model = trained_models[best_model_name]\n",
        "    best_predictions = predictions[best_model_name]\n",
        "\n",
        "\n",
        "    return best_model, predictions_df, model_performances, feature_names\n",
        "\n",
        "# Function to load data from file\n",
        "def load_from_uploaded_file(file_path):\n",
        "    \"\"\"Load patient data from a CSV file with headers\"\"\"\n",
        "    try:\n",
        "        # Read the file with its headers\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # List of expected columns\n",
        "        expected_columns = ['PatientNum', 'PatientAgeAtAdmission', 'TotalHospitalDays',\n",
        "                           'Execution_Date', 'HoursFromAdmission', 'Parameter_Name',\n",
        "                           'ResultValue', 'NumericResult', 'ReadingSequence']\n",
        "\n",
        "        # Check if all required columns are present\n",
        "        missing_columns = [col for col in expected_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing expected columns: {missing_columns}\")\n",
        "            print(f\"Available columns: {df.columns.tolist()}\")\n",
        "\n",
        "        # Convert numeric columns\n",
        "        numeric_columns = ['PatientNum', 'PatientAgeAtAdmission', 'TotalHospitalDays',\n",
        "                          'HoursFromAdmission', 'NumericResult']\n",
        "\n",
        "        for col in numeric_columns:\n",
        "            if col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        # Last attempt - try to read a few lines as plain text\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for i in range(5):\n",
        "                    print(f.readline().strip())\n",
        "        except:\n",
        "            pass\n",
        "        return None\n",
        "\n",
        "# Main execution function\n",
        "def main(vital_signs_file='over_20_records.csv', diagnoses_file='patients_diagnoses.csv', patterns_file='train.csv'):\n",
        "    \"\"\"Main function to load data and run the prediction model\"\"\"\n",
        "    # Load the vital signs data\n",
        "    try:\n",
        "        df = load_from_uploaded_file(vital_signs_file)\n",
        "    except Exception as e:\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Load temporal patterns data\n",
        "    try:\n",
        "        patterns_df = load_temporal_patterns(patterns_file)\n",
        "        if patterns_df is None:\n",
        "            return None, None, None, None\n",
        "    except Exception as e:\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Load diagnoses data if file provided\n",
        "    diagnoses_df = None\n",
        "    if diagnoses_file:\n",
        "        try:\n",
        "            diagnoses_df = pd.read_csv(diagnoses_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading diagnoses data: {e}\")\n",
        "\n",
        "    if df is not None and patterns_df is not None:\n",
        "        # Run the prediction model with all features\n",
        "        best_model, predictions_df, model_performances, feature_names = predict_hospitalization_days(df, patterns_df, diagnoses_df)\n",
        "\n",
        "        # Display results\n",
        "\n",
        "        return best_model, predictions_df, model_performances, feature_names\n",
        "    else:\n",
        "        print(\"Failed to load required data. Please check the file formats.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Run the model\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the model with vital signs, diagnoses data, and temporal patterns\n",
        "    best_model, predictions_df, model_performances, feature_names = main(\"over_20_records.csv\", \"patients_diagnoses.csv\", \"train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4QP1OnG_yQl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rBMDAerYyE_9"
      }
    }
  ]
}